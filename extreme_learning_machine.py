# -*- coding: utf-8 -*-
"""Extreme Learning Machine - 24 de Fevereiro de 2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AtyonrtksAD5CZM_nc_693b2NeofiZd1
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon

from sklearn.datasets import load_breast_cancer, make_blobs, make_moons
from sklearn.model_selection import train_test_split

# Classificadores
from sklearn.linear_model import Perceptron

# Biblioteca para criar classificador.
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn import preprocessing
from sklearn.impute import SimpleImputer

# Biblioteca para visualização.
from mlxtend.plotting import plot_decision_regions

import math

"""## Duas Luas"""

X, y = make_moons(n_samples=1000, noise=.1, random_state=1)
y = 2*y-1

class HLM_ELM(BaseEstimator, ClassifierMixin):
    '''
        M = number of morph units
        L = number of linear units
    '''
    
    def __init__(self, M = 10, L = 10):
        self.M = M
        self.L = L
    
    def fit(self, X, y):
        '''
            Obtain feature space H (output of hidden layer)
            and solve alpha throught minimizing the approximation error in the squared error sense:
        '''
        self.le = preprocessing.LabelEncoder()
        self.le.fit(y)
        y = self.le.transform(y)
        self.set_morph_weights(X)
        self.set_linear_weights(X)
        

        bias = np.ones((X.shape[0],1))

        H1 = self.morphLayer(X)
        H2 = self.linearLayer(X)
        H = np.hstack([bias,H1,H2])
        self.alpha_ = np.linalg.lstsq(H, 2*y - 1, rcond=None)[0]
    
    def predict(self,X):
        pred = self.decision_function(X) >= 0
        ypred = np.asarray([int(y) for y in pred])
        return self.le.inverse_transform(ypred)
    
    def decision_function(self, X):
        bias = np.ones((X.shape[0],1))

        H1 = self.morphLayer(X)
        H2 = self.linearLayer(X)
        H = np.hstack([bias,H1,H2])
        
        return np.dot(H,self.alpha_)

    def erosion(self,X, w):
        return np.min(X+w, axis=1)

    def set_morph_weights(self,X):
        '''
            Return the weights(boxes) randomly obtained in the interval [a,b] of X, for the morph side of the layer.
        '''
        self.boxes_ = list()
        a = np.min(X,axis=0)
        b = np.max(X,axis=0)
        for i in range(self.M):
            # first point
            t = np.random.rand(X.shape[1],)
            p1 = a*(1-t)+b*t
            # second point
            t = np.random.rand(X.shape[1],)
            p2 = a*(1-t)+b*t
            self.boxes_.append(np.hstack([-np.minimum(p1,p2), np.maximum(p1,p2)]))

    def set_linear_weights(self, X):
        '''
            Return the weights randomly obtained for the linear side of the layer.
        '''
        self.U = np.random.randn(self.L,X.shape[1])
        self.bias = np.random.randn(self.L,)

    def morphLayer(self,X):
        '''
            Apply erosion to morph weights (u and v concatenated). 
            No activation function needed since erosion is already a non-linear function
        '''
        X = np.hstack([X,-X])
        return np.vstack([self.erosion(X, uv) for uv in self.boxes_]).T

    def linearLayer(self,X):
        '''
            Apply sigmoid function in the inner product of X and weights + bias
        '''
        z = np.dot(X,self.U.T) + self.bias
        return self.sigmoid(z)

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

# clf = HLM_ELM(M=10, L=15)
# clf.fit(X, y)

# plot_decision_regions(X,y, clf)

